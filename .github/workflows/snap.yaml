name: Build Spark client snap and run tests

env:
  RELEASE: edge
  BRANCH: dev

on:
  push:
    branches:
      - ${{ env.BRANCH }}
  pull_request:
    branches:
      - ${{ env.BRANCH }}
  on:
    schedule:
      - cron: '30 10 * * *'

jobs:
  build-condition:
    name: Check build condition for Snap
    runs-on: ubuntu-latest
    steps:
      - id: published-version
        name: Read last published stable version
        uses: juliangruber/read-file-action@v1
        with:
          path: ../../VERSION
      - id: latest-version
        name: Check latest stable version available for download
        run: |
          echo "LATEST_STABLE_SPARK_VERSION=$(curl --silent https://downloads.apache.org/spark/ | grep "spark-" | cut -d'>' -f3 | cut -d'/' -f1  | sort | tail -n 1)" >> $GITHUB_ENV
          echo "CURRENT_PUBLISHED_VERSION=${{ steps.published-version.outputs.content }}" >> $GITHUB_ENV
          echo "STATUSCODE=$(curl --silent --head https://downloads.apache.org/spark/${{ env.LATEST_STABLE_SPARK_VERSION }}/${{ env.LATEST_STABLE_SPARK_VERSION }}-bin-hadoop3.tgz | head -n 1 | cut -d' ' -f2)" >> $GITHUB_ENV
          echo "====================================="
          echo ${{ env.LATEST_STABLE_SPARK_VERSION }}
          echo ${{ env.CURRENT_PUBLISHED_VERSION }}
          echo ${{ env.STATUSCODE }}
          echo "====================================="
          if  [[ ${{ env.LATEST_STABLE_SPARK_VERSION }} -ne ${{ env.CURRENT_PUBLISHED_VERSION }} ]] && [[ ${{ env.STATUSCODE }} -eq 200 ]]
            then 
              echo "BUILD_AND_PUBLISH_DECISION=1" >> $GITHUB_ENV
            else
              echo "BUILD_AND_PUBLISH_DECISION=0" >> $GITHUB_ENV
          fi
          echo "====================================="
          echo ${{ env.BUILD_AND_PUBLISH_DECISION }}
          echo "====================================="

  build:
    name: Build Snap
    if: ${{ env.BUILD_AND_PUBLISH_DECISION }}  ==  "1" || github.event_name == "push" || github.event_name == "pull_request"
    runs-on: ubuntu-latest
    needs:
      - build-condition
    outputs:
      snap-file: ${{ steps.build-snap.outputs.snap }}
    steps:
      - id: checkout
        name: Checkout repo
        uses: actions/checkout@v3
        with:
          ref: dev
      - id: build-snap
        name: Build snap
        uses: snapcore/action-build@v1
        with:
          snapcraft-channel: 7.x/candidate
      - id: upload
        name: Upload built snap job artifact
        uses: actions/upload-artifact@v3
        with:
          name: spark-client_snap_amd64
          path: spark-client_${{ env.LATEST_STABLE_SPARK_VERSION }}_amd64.snap

  test:
    name: Test Snap
    if: ${{ env.BUILD_AND_PUBLISH_DECISION }}  ==  "1" || github.event_name == "push" || github.event_name == "pull_request"
    runs-on: ubuntu-latest
    needs:
      - build-condition
      - build
    steps:
      - name: Download snap file
        uses: actions/download-artifact@v3
        with:
          name: spark-client_snap_amd64
          path: .

      - name: Install snap file
        run: |
          sudo snap install spark-client_${{ env.LATEST_STABLE_SPARK_VERSION }}_amd64.snap --dangerous

      - name: Setup Spark client
        run: |
          # connect interfaces
          sudo snap connect spark-client:enable-kubeconfig-access
          sudo snap connect spark-client:enable-scala-history
          
          # setup pre requisites
          sudo snap install microk8s --classic
          sudo microk8s status --wait-ready
          sudo snap alias microk8s.kubectl kubectl
          sudo usermod -a -G microk8s runner
          mkdir -p /home/runner/.kube
          newgrp microk8s
          sudo microk8s config > /home/runner/.kube/config
          sudo chown -f -R runner /home/runner/.kube
          sudo microk8s.enable dns
          
          # create the service account
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          service-account \
          spark
          
          # create the ca certificate
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          get-ca-cert > /home/runner/ca.crt
                    
          # create the oauth token
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          get-token \
          spark > /home/runner/token

      - name: Run example job
        run: |
          K8S_MASTER_URL=k8s://$(sudo kubectl --kubeconfig=/home/runner/.kube/config config view -o jsonpath="{.clusters[0]['cluster.server']}")
          CA_CRT_FILE='/home/runner/ca.crt'
          OAUTH_TOKEN_FILE='/home/runner/token'
          SPARK_CONTAINER_IMAGE='docker.io/averma32/sparkpy6:latest'
          NAMESPACE='default'
          SERVICE_ACCOUNT_NAME='spark'
          SPARK_EXAMPLES_JAR_NAME='spark-examples_2.12-3.4.0-SNAPSHOT.jar'
          
          # run the sample pi job using spark-submit
          spark-client.spark-submit \
          --master $K8S_MASTER_URL \
          --deploy-mode cluster \
          --name spark-pi \
          --conf spark.kubernetes.authenticate.submission.caCertFile=$CA_CRT_FILE \
          --conf spark.kubernetes.authenticate.submission.oauthTokenFile=$OAUTH_TOKEN_FILE \
          --conf spark.executor.instances=1 \
          --conf spark.kubernetes.container.image=$SPARK_CONTAINER_IMAGE \
          --conf spark.kubernetes.container.image.pullPolicy=Always \
          --conf spark.kubernetes.namespace=$NAMESPACE \
          --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SERVICE_ACCOUNT_NAME \
          --conf spark.eventLog.enabled=false \
          --conf spark.kubernetes.driver.request.cores=100m \
          --conf spark.kubernetes.executor.request.cores=100m \
          --class org.apache.spark.examples.SparkPi \
          local:///opt/spark/examples/jars/$SPARK_EXAMPLES_JAR_NAME 100
          
          sudo kubectl --kubeconfig=/home/runner/.kube/config get pods
          DRIVER_JOB=$(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | grep driver | tail -n 1 | cut -d' ' -f1)
          echo -e "Inspecting logs for driver job: ${DRIVER_JOB}"
          sudo kubectl --kubeconfig=/home/runner/.kube/config logs ${DRIVER_JOB}
          
          EXECUTOR_JOB=$(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | grep exec | tail -n 1 | cut -d' ' -f1)
          echo -e "Inspecting state of executor job: ${EXECUTOR_JOB}"
          sudo kubectl --kubeconfig=/home/runner/.kube/config describe pod ${EXECUTOR_JOB}
          
          # Check job output
          pi=$(sudo kubectl --kubeconfig=/home/runner/.kube/config logs $(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | tail -n 1 | cut -d' ' -f1)  | grep 'Pi is roughly' | rev | cut -d' ' -f1 | rev | cut -c 1-4)
          echo -e "Spark Pi Job Output: \n ${pi}"
          if [ "${pi}" != "3.14" ]; then
              exit 1
          fi

  publish:
    name: Publish Snap
    if: ${{ env.BUILD_AND_PUBLISH_DECISION }}  ==  "1"
    runs-on: ubuntu-latest
    needs:
      - build-condition
      - build
      - test
    steps:
      - name: Download snap file
        uses: actions/download-artifact@v3
        with:
          name: spark-client_snap_amd64
          path: .
      - name: Publish snap to Store
        uses: snapcore/action-publish@v1
        env:
          SNAPCRAFT_STORE_CREDENTIALS: ${{ secrets.STORE_LOGIN }}
        with:
          snap: spark-client_${{ env.LATEST_STABLE_SPARK_VERSION }}_amd64.snap
          release: ${{ env.RELEASE }}
      - name: Record the published version
        uses: DamianReeves/write-file-action@master
        with:
          path: ../../VERSION
          write-mode: overwrite
          contents: |
            ${{ env.LATEST_STABLE_SPARK_VERSION }}
