name: Build Spark client snap and run tests

env:
  VERSION: 0.1
  RELEASE: edge

on:
  push:
    branches:
      - dev

jobs:
  build:
    name: Build Snap
    runs-on: ubuntu-latest
    outputs:
      snap-file: ${{ steps.build-snap.outputs.snap }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          ref: dev

      - id: build-snap
        name: Build snap
        uses: snapcore/action-build@v1
        with:
          snapcraft-channel: 7.x/candidate

      - name: Upload built snap job artifact
        uses: actions/upload-artifact@v3
        with:
          name: spark-client_snap_amd64
          path: spark-client_${{env.VERSION}}_amd64.snap

  test:
    name: Test Snap
    runs-on: ubuntu-latest
    needs:
      - build
    steps:
      - name: Download snap file
        uses: actions/download-artifact@v3
        with:
          name: spark-client_snap_amd64
          path: .

      - name: Install snap file
        run: |
          sudo snap install spark-client_${{env.VERSION}}_amd64.snap --dangerous

      - name: Setup Spark client
        run: |
          # connect interfaces
          sudo snap connect spark-client:enable-kubeconfig-access
          sudo snap connect spark-client:enable-scala-history
          
          # setup pre requisites
          sudo snap install microk8s --classic
          sudo microk8s status --wait-ready
          sudo snap alias microk8s.kubectl kubectl
          sudo usermod -a -G microk8s runner
          mkdir -p /home/runner/.kube
          newgrp microk8s
          sudo microk8s config > /home/runner/.kube/config
          sudo chown -f -R runner /home/runner/.kube
          sudo microk8s.enable dns
          
          # create the service account
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          service-account \
          spark
          
          # create the ca certificate
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          get-ca-cert > /home/runner/ca.crt
                    
          # create the oauth token
          spark-client.setup-spark-k8s \
          --kubeconfig /home/runner/.kube/config \
          --cluster microk8s-cluster \
          get-token \
          spark > /home/runner/token

      - name: Run example job
        run: |
          K8S_MASTER_URL=k8s://$(sudo kubectl --kubeconfig=/home/runner/.kube/config config view -o jsonpath="{.clusters[0]['cluster.server']}")
          CA_CRT_FILE='/home/runner/ca.crt'
          OAUTH_TOKEN_FILE='/home/runner/token'
          SPARK_CONTAINER_IMAGE='docker.io/averma32/sparkpy6:latest'
          NAMESPACE='default'
          SERVICE_ACCOUNT_NAME='spark'
          SPARK_EXAMPLES_JAR_NAME='spark-examples_2.12-3.4.0-SNAPSHOT.jar'
          
          # run the sample pi job using spark-submit
          spark-client.spark-submit \
          --master $K8S_MASTER_URL \
          --deploy-mode cluster \
          --name spark-pi \
          --conf spark.kubernetes.authenticate.submission.caCertFile=$CA_CRT_FILE \
          --conf spark.kubernetes.authenticate.submission.oauthTokenFile=$OAUTH_TOKEN_FILE \
          --conf spark.executor.instances=1 \
          --conf spark.kubernetes.container.image=$SPARK_CONTAINER_IMAGE \
          --conf spark.kubernetes.container.image.pullPolicy=Always \
          --conf spark.kubernetes.namespace=$NAMESPACE \
          --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SERVICE_ACCOUNT_NAME \
          --conf spark.eventLog.enabled=false \
          --conf spark.kubernetes.driver.request.cores=100m \
          --conf spark.kubernetes.executor.request.cores=100m \
          --class org.apache.spark.examples.SparkPi \
          local:///opt/spark/examples/jars/$SPARK_EXAMPLES_JAR_NAME 100
          
          sudo kubectl --kubeconfig=/home/runner/.kube/config get pods
          DRIVER_JOB=$(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | grep driver | tail -n 1 | cut -d' ' -f1)
          echo -e "Inspecting logs for driver job: ${DRIVER_JOB}"
          sudo kubectl --kubeconfig=/home/runner/.kube/config logs ${DRIVER_JOB}
          
          EXECUTOR_JOB=$(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | grep exec | tail -n 1 | cut -d' ' -f1)
          echo -e "Inspecting state of executor job: ${EXECUTOR_JOB}"
          sudo kubectl --kubeconfig=/home/runner/.kube/config describe pod ${EXECUTOR_JOB}
          
          # Check job output
          pi=$(sudo kubectl --kubeconfig=/home/runner/.kube/config logs $(sudo kubectl --kubeconfig=/home/runner/.kube/config get pods | tail -n 1 | cut -d' ' -f1)  | grep 'Pi is roughly' | rev | cut -d' ' -f1 | rev | cut -c 1-4)
          echo -e "Spark Pi Job Output: \n ${pi}"
          if [ "${pi}" != "3.14" ]; then
              exit 1
          fi
